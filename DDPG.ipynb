{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f82ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "from xml.etree.ElementTree import tostring\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as TNN\n",
    "import torch.nn.functional as TF\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb39bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数定义\n",
    "ENV_NAME = 'MountainCarContinuous-v0'\n",
    "BUFFER_SIZE = 1000000\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCHSIZE = 64\n",
    "TEST = 5\n",
    "SAVINGPATH = \"./modelDDPG/\"\n",
    "EPISODE = 10000\n",
    "TAU = 0.001\n",
    "STEP = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a32941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU噪声生成\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "\tdef __init__(self, action_dim, mu = 0, theta = 0.15, sigma = 0.2):\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\tself.mu = mu\n",
    "\t\tself.theta = theta\n",
    "\t\tself.sigma = sigma\n",
    "\t\tself.X = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.X = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "\tdef sample(self):\n",
    "\t\tdx = self.theta * (self.mu - self.X)\n",
    "\t\tdx = dx + self.sigma * np.random.randn(len(self.X))\n",
    "\t\tself.X = self.X + dx\n",
    "\t\treturn torch.Tensor(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4823e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReplayBuffer\n",
    "class ReplayBuffer():\n",
    "    def __init__(self,env,buffersize):\n",
    "        self.buffer = deque(maxlen=buffersize)\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "    def append(self,content):\n",
    "        self.buffer.append([content[0],content[1],content[2],content[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e522af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样minibatch\n",
    "# 生成标签 y 及 minibatch\n",
    "def sample_Batch(replaybuffer,Q_t,u_t,batch_size=BATCHSIZE):\n",
    "    # 根据BATCHSIZE采样\n",
    "    minibatch = random.sample(replaybuffer.buffer,batch_size)\n",
    "    \n",
    "    state_batch = [data[0] for data in minibatch]\n",
    "    action_batch = [data[1] for data in minibatch]\n",
    "    reward_batch = [data[2] for data in minibatch]\n",
    "    next_state_batch = [data[3] for data in minibatch]\n",
    "\n",
    "    #将数据取出转换为成为张量\n",
    "    tensor_state = torch.Tensor(state_batch).reshape(batch_size,Q_t.state_dim)\n",
    "    tensor_action = torch.Tensor(action_batch).reshape(batch_size,Q_t.action_dim)\n",
    "    tensor_r = torch.Tensor(reward_batch).reshape(batch_size,1)\n",
    "    tensor_nextstate = torch.Tensor(next_state_batch).reshape(batch_size,Q_t.state_dim)\n",
    "\n",
    "    tensor_u_t = u_t.net(tensor_nextstate)\n",
    "    # 将数据送入Q_target计算\n",
    "    tensor_y = tensor_r + GAMMA * Q_t(tensor_nextstate,tensor_u_t)\n",
    "\n",
    "    return tensor_y,tensor_action,tensor_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a334292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数化策略网络\n",
    "# 输入为状态state，输出为确定的动作\n",
    "class Policy_Net(TNN.Module):\n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.bound = env.action_space.high\n",
    "        self.bound_low = env.action_space.low\n",
    "        self.net = TNN.Sequential()\n",
    "        self.net.add_module(\"fc1\",TNN.Linear(self.state_dim,30))\n",
    "        self.net.add_module(\"relu1\",TNN.ReLU())\n",
    "        self.net.add_module(\"fc2\",TNN.Linear(30,20))\n",
    "        self.net.add_module(\"relu2\",TNN.ReLU())\n",
    "        self.net.add_module(\"fc3\",TNN.Linear(20,self.action_dim))\n",
    "        self.net.add_module(\"tanh1\",TNN.Tanh())\n",
    "        # 初始化最后一层\n",
    "        TNN.init.uniform_(self.net.fc3.weight,a=-0.003,b=0.003)\n",
    "        TNN.init.uniform_(self.net.fc3.bias,a=-0.003,b=0.003)\n",
    "        # 初始化OU噪声\n",
    "        self.OU_Noise = OrnsteinUhlenbeckActionNoise(self.action_dim)\n",
    "        self.optimizer = torch.optim.Adam([{'params':self.net.parameters()}],lr=0.0001)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        Actions = self.net(x)\n",
    "        return Actions\n",
    "    \n",
    "    def action(self,state):\n",
    "        state = torch.Tensor(state)\n",
    "        a=self.forward(state)\n",
    "        return a * self.bound.item()\n",
    "\n",
    "    #带OU噪声的action\n",
    "    def action_with_noise(self,state):\n",
    "        state = torch.Tensor(state)\n",
    "        ret = self.action(state) + self.OU_Noise.sample()\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe31247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Net\n",
    "# 输入为状态state和动作action，输出为价值\n",
    "class Critic_Net(TNN.Module):\n",
    "    def __init__(self,env) :\n",
    "        super().__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0] \n",
    "\n",
    "        # 隐层网络定义\n",
    "        self.fc1 = TNN.Linear(self.state_dim,20)\n",
    "        self.fc2 = TNN.Linear(20+self.action_dim,20)\n",
    "        self.fc3 = TNN.Linear(20,1)\n",
    "        # 初始化最后一层\n",
    "        TNN.init.uniform_(self.fc3.weight,a=0.0003,b=0.0003)\n",
    "        TNN.init.uniform_(self.fc3.bias,a=0.0003,b=0.0003)\n",
    "\n",
    "        self.train_setup()\n",
    "\n",
    "    def forward(self,state,action):\n",
    "        h1 = TF.relu(self.fc1(state))\n",
    "        #print(x)\n",
    "        #print(action)\n",
    "        cat = torch.cat((h1,action),axis=1)\n",
    "        h2 = TF.relu(self.fc2(cat))\n",
    "        out = self.fc3(h2)\n",
    "        return out\n",
    "\n",
    "    # 定义训练优化器和损失函数\n",
    "    def train_setup(self):\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                    {'params':self.fc1.parameters()},\n",
    "                    {'params':self.fc2.parameters()},\n",
    "                    {'params':self.fc3.parameters()}],lr=0.001)\n",
    "                    \n",
    "        self.loss_fn = TNN.MSELoss()\n",
    "\n",
    "    # 根据targetQ和targetU生成的y训练Critic网络\n",
    "    def update_criticNet(self,tensor_state,tensor_action,tensor_y):\n",
    "        x = self.forward(tensor_state,tensor_action)\n",
    "        loss = self.loss_fn(x,tensor_y)\n",
    "\n",
    "        # 反向传播\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1119c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从replaybuffer中采样，完成算法的一次迭代\n",
    "def update_Network(replaybuffer,Q,Q_t,u,u_t):\n",
    "    if len(replaybuffer.buffer)<BATCHSIZE:\n",
    "        return \n",
    "    # 形成 minibatch\n",
    "    tensor_y,tensor_action,tensor_state  = sample_Batch(replaybuffer,Q_t,u_t)\n",
    "\n",
    "    ## update Critic网络\n",
    "    Q.update_criticNet(tensor_state,tensor_action,tensor_y)\n",
    "\n",
    "    ## update Actor网络\n",
    "    # 使用Actor生成动作 u_action\n",
    "    u_action = u(tensor_state)\n",
    "    # 将采样的状态state和Actor生成的动作输入Critic网络计算评判结果\n",
    "    loss_u_Grad = - Q(tensor_state,u_action)\n",
    "    # 损失函数为取平均，对Q的评判结果的平均值取负，因为要做的是梯度上升\n",
    "    #loss_u_Grad =   Q_critic\n",
    "    loss_u_Grad = loss_u_Grad.mean()\n",
    "    \n",
    "    # 将反向传播通路上的Q网络的节点梯度清零\n",
    "    Q.optimizer.zero_grad()\n",
    "    # 反向传播\n",
    "    u.optimizer.zero_grad()\n",
    "    loss_u_Grad.backward()\n",
    "    u.optimizer.step()\n",
    "\n",
    "    ### update target网络\n",
    "    ## update Q_target网络\n",
    "    update_tNet(Q,Q_t,TAU)\n",
    "    \n",
    "    ## 更新u_target网络\n",
    "    update_tNet(u,u_t,TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新target网络，输入tau为更新权重\n",
    "def update_tNet(Net,Net_t,tau):\n",
    "    # 用于target网络的初始化\n",
    "    if tau == 0:\n",
    "        Net_t.load_state_dict(Net.state_dict())\n",
    "        return\n",
    "    \n",
    "    state_dict = Net.state_dict()\n",
    "    state_dict_t = Net_t.state_dict()\n",
    "    #print(state_dict)\n",
    "    for key in state_dict:\n",
    "        state_dict_t[key] = state_dict[key]*tau + (1-tau)*state_dict_t[key]\n",
    "    Net_t.load_state_dict(state_dict_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d27c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 Evaluation Average Reward: -0.00018443086012521413\n",
      "episode:  100 Evaluation Average Reward: -0.12882516758247425\n",
      "episode:  200 Evaluation Average Reward: -0.12119056573987108\n",
      "episode:  300 Evaluation Average Reward: -0.005951791737562561\n",
      "episode:  400 Evaluation Average Reward: -0.039328653384987235\n",
      "episode:  500 Evaluation Average Reward: -0.047719049670496436\n",
      "episode:  600 Evaluation Average Reward: -0.12340840088010363\n",
      "episode:  700 Evaluation Average Reward: -0.7863463507293721\n",
      "episode:  800 Evaluation Average Reward: -1.3954676846445642\n",
      "episode:  900 Evaluation Average Reward: -12.735283124272883\n",
      "episode:  1000 Evaluation Average Reward: -19.233619675667047\n",
      "episode:  1100 Evaluation Average Reward: -19.982987305858238\n",
      "episode:  1200 Evaluation Average Reward: 91.31608152489275\n",
      "episode:  1300 Evaluation Average Reward: 88.85023502767196\n",
      "episode:  1400 Evaluation Average Reward: -19.99999278307167\n",
      "episode:  1500 Evaluation Average Reward: -18.156353624002385\n",
      "episode:  1600 Evaluation Average Reward: -19.999845359969466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15076\\1722867182.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15076\\1722867182.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;31m# 产生动作\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_with_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                 \u001b[1;31m#观测\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15076\\2724651918.py\u001b[0m in \u001b[0;36maction_with_noise\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maction_with_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOU_Noise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15076\\2724651918.py\u001b[0m in \u001b[0;36maction\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15076\\2724651918.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mActions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mActions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch_gym\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch_gym\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch_gym\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch_gym\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch_gym\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1676\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1677\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    Mode = 'Train'\n",
    "    env = gym.make(ENV_NAME)\n",
    "    replaybuffer = ReplayBuffer(env,BUFFER_SIZE)\n",
    "    # 创建网络\n",
    "    Q = Critic_Net(env)\n",
    "    u = Policy_Net(env)\n",
    "    # target\n",
    "    Q_t = Critic_Net(env)\n",
    "    u_t = Policy_Net(env)\n",
    "\n",
    "    # 初始化target\n",
    "    update_tNet(Q,Q_t,0)\n",
    "    update_tNet(u,u_t,0)\n",
    "    if Mode == 'Train':\n",
    "        ave_reward = []\n",
    "        for episode in range(EPISODE):\n",
    "            # 初始化环境\n",
    "            state = env.reset()\n",
    "            acc_reward = 0 #累积奖赏\n",
    "            # 训练\n",
    "            for step in range(STEP):\n",
    "                # 产生动作\n",
    "                action = u.action_with_noise(state)\n",
    "                #观测\n",
    "                next_state,reward,done,_ = env.step(action.detach().numpy())\n",
    "                replaybuffer.append([state,action,reward,next_state])\n",
    "                #更新四个网络\n",
    "                update_Network(replaybuffer,Q,Q_t,u,u_t)\n",
    "                state = next_state\n",
    "                if done : \n",
    "                    break\n",
    "            if episode%100 == 0:#测试\n",
    "                acc_reward = 0\n",
    "                for i in range(TEST):\n",
    "                    state = env.reset()\n",
    "                    for step in range(STEP):\n",
    "                        action = u.action(state)\n",
    "                        state,reward,done,_ = env.step(action.detach().numpy())\n",
    "                        acc_reward += reward\n",
    "                        if done:\n",
    "                            break\n",
    "                ave_reward.append(acc_reward/TEST)\n",
    "                print('episode: ',episode,'Evaluation Average Reward:',acc_reward/TEST)\n",
    "                if acc_reward/TEST >93:\n",
    "                    #保存\n",
    "                    torch.save(u.state_dict(),SAVINGPATH+\"model-\"+str(acc_reward/TEST)+\".pth\")                \n",
    "                    break\n",
    "            \n",
    "    else :\n",
    "        u.load_state_dict(torch.load(\"modelDDPG\\model-94.23124390777878.pth\"))\n",
    "        state = env.reset()\n",
    "        acc_reward = 0\n",
    "        while True:\n",
    "            action = u.action(state)\n",
    "            state,reward,done,_ = env.step(action.detach().numpy())\n",
    "            acc_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(\"total reward: {}\".format(acc_reward))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ddd487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 92.5034822225326\n",
      "total reward: 94.210850786879\n",
      "total reward: 94.38004282103113\n",
      "total reward: 94.36252076554238\n",
      "total reward: 94.24541889939272\n",
      "total reward: 91.10457492914891\n",
      "total reward: 93.34046744742642\n",
      "total reward: 93.49390179729043\n",
      "total reward: 94.35951246150044\n",
      "total reward: 94.22223874419761\n",
      "total reward: 94.24694256858206\n",
      "total reward: 94.5733744021846\n",
      "total reward: 94.24389989921806\n",
      "total reward: 91.0324431312632\n",
      "total reward: 94.07632175582212\n",
      "total reward: 93.75615330343385\n",
      "total reward: 94.3659897627396\n",
      "total reward: 94.57191740453176\n",
      "total reward: 89.36962892967787\n",
      "total reward: 93.70021643669196\n",
      "total reward: 94.25243958122559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14916\\2584200406.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14916\\2584200406.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0macc_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch_gym\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch_gym\\lib\\site-packages\\gym\\envs\\classic_control\\continuous_mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcartrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"rgb_array\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch_gym\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch_gym\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_always_dwm\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dwm_composition_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                     \u001b[0m_dwmapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDwmFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    env = gym.make(ENV_NAME)\n",
    "    # 创建网络\n",
    "    u = Policy_Net(env)\n",
    "    u.load_state_dict(torch.load(\"modelDDPG\\model-94.23124390777878.pth\"))\n",
    "    state = env.reset()\n",
    "    acc_reward = 0\n",
    "    while True:\n",
    "        action = u.action(state)\n",
    "        state,reward,done,_ = env.step(action.detach().numpy())\n",
    "        acc_reward += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            print(\"total reward: {}\".format(acc_reward))\n",
    "            acc_reward=0\n",
    "    #\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f9026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
