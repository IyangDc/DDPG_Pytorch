{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f82ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "from xml.etree.ElementTree import tostring\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as TNN\n",
    "import torch.nn.functional as TF\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb39bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数定义\n",
    "ENV_NAME = 'MountainCarContinuous-v0'\n",
    "BUFFER_SIZE = 1000000\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCHSIZE = 64\n",
    "TEST = 5\n",
    "SAVINGPATH = \"./modelDDPG/\"\n",
    "EPISODE = 10000\n",
    "TAU = 0.001\n",
    "STEP = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a32941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OU噪声生成\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "\tdef __init__(self, action_dim, mu = 0, theta = 0.15, sigma = 0.2):\n",
    "\t\tself.action_dim = action_dim\n",
    "\t\tself.mu = mu\n",
    "\t\tself.theta = theta\n",
    "\t\tself.sigma = sigma\n",
    "\t\tself.X = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.X = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "\tdef sample(self):\n",
    "\t\tdx = self.theta * (self.mu - self.X)\n",
    "\t\tdx = dx + self.sigma * np.random.randn(len(self.X))\n",
    "\t\tself.X = self.X + dx\n",
    "\t\treturn torch.Tensor(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4823e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReplayBuffer\n",
    "class ReplayBuffer():\n",
    "    def __init__(self,env,buffersize):\n",
    "        self.buffer = deque(maxlen=buffersize)\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "    def append(self,content):\n",
    "        self.buffer.append([content[0],content[1],content[2],content[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e522af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采样minibatch\n",
    "# 生成标签 y 及 minibatch\n",
    "def sample_Batch(replaybuffer,Q_t,u_t,batch_size=BATCHSIZE):\n",
    "    # 根据BATCHSIZE采样\n",
    "    minibatch = random.sample(replaybuffer.buffer,batch_size)\n",
    "    \n",
    "    state_batch = [data[0] for data in minibatch]\n",
    "    action_batch = [data[1] for data in minibatch]\n",
    "    reward_batch = [data[2] for data in minibatch]\n",
    "    next_state_batch = [data[3] for data in minibatch]\n",
    "\n",
    "    #将数据取出转换为成为张量\n",
    "    tensor_state = torch.Tensor(state_batch).reshape(batch_size,Q_t.state_dim)\n",
    "    tensor_action = torch.Tensor(action_batch).reshape(batch_size,Q_t.action_dim)\n",
    "    tensor_r = torch.Tensor(reward_batch).reshape(batch_size,1)\n",
    "    tensor_nextstate = torch.Tensor(next_state_batch).reshape(batch_size,Q_t.state_dim)\n",
    "\n",
    "    tensor_u_t = u_t.net(tensor_nextstate)\n",
    "    # 将数据送入Q_target计算\n",
    "    tensor_y = tensor_r + GAMMA * Q_t(tensor_nextstate,tensor_u_t)\n",
    "\n",
    "    return tensor_y,tensor_action,tensor_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a334292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数化策略网络\n",
    "# 输入为状态state，输出为确定的动作\n",
    "class Policy_Net(TNN.Module):\n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.bound = env.action_space.high\n",
    "        self.bound_low = env.action_space.low\n",
    "        self.net = TNN.Sequential()\n",
    "        self.net.add_module(\"fc1\",TNN.Linear(self.state_dim,30))\n",
    "        self.net.add_module(\"relu1\",TNN.ReLU())\n",
    "        self.net.add_module(\"fc2\",TNN.Linear(30,20))\n",
    "        self.net.add_module(\"relu2\",TNN.ReLU())\n",
    "        self.net.add_module(\"fc3\",TNN.Linear(20,self.action_dim))\n",
    "        self.net.add_module(\"tanh1\",TNN.Tanh())\n",
    "        # 初始化最后一层\n",
    "        TNN.init.uniform_(self.net.fc3.weight,a=-0.003,b=0.003)\n",
    "        TNN.init.uniform_(self.net.fc3.bias,a=-0.003,b=0.003)\n",
    "        # 初始化OU噪声\n",
    "        self.OU_Noise = OrnsteinUhlenbeckActionNoise(self.action_dim)\n",
    "        self.optimizer = torch.optim.Adam([{'params':self.net.parameters()}],lr=0.0001)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        Actions = self.net(x)\n",
    "        return Actions\n",
    "    \n",
    "    def action(self,state):\n",
    "        state = torch.Tensor(state)\n",
    "        a=self.forward(state)\n",
    "        return a * self.bound.item()\n",
    "\n",
    "    #带OU噪声的action\n",
    "    def action_with_noise(self,state):\n",
    "        state = torch.Tensor(state)\n",
    "        ret = self.action(state) + self.OU_Noise.sample()\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe31247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Net\n",
    "# 输入为状态state和动作action，输出为价值\n",
    "class Critic_Net(TNN.Module):\n",
    "    def __init__(self,env) :\n",
    "        super().__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0] \n",
    "\n",
    "        # 隐层网络定义\n",
    "        self.fc1 = TNN.Linear(self.state_dim,20)\n",
    "        self.fc2 = TNN.Linear(20+self.action_dim,20)\n",
    "        self.fc3 = TNN.Linear(20,1)\n",
    "        # 初始化最后一层\n",
    "        TNN.init.uniform_(self.fc3.weight,a=0.0003,b=0.0003)\n",
    "        TNN.init.uniform_(self.fc3.bias,a=0.0003,b=0.0003)\n",
    "\n",
    "        self.replaybuffer = deque(maxlen=BUFFER_SIZE)\n",
    "        self.ite=0\n",
    "        self.train_setup()\n",
    "\n",
    "    def forward(self,state,action):\n",
    "        h1 = TF.relu(self.fc1(state))\n",
    "        #print(x)\n",
    "        #print(action)\n",
    "        cat = torch.cat((h1,action),axis=1)\n",
    "        h2 = TF.relu(self.fc2(cat))\n",
    "        out = self.fc3(h2)\n",
    "        return out\n",
    "\n",
    "    # 定义训练优化器和损失函数\n",
    "    def train_setup(self):\n",
    "        self.optimizer = torch.optim.Adam([{'params':self.fc1.parameters()},{'params':self.fc2.parameters()},{'params':self.fc3.parameters()}],lr=0.001)\n",
    "        self.loss_fn = TNN.MSELoss()\n",
    "\n",
    "    # 根据targetQ和targetU生成的y训练Critic网络\n",
    "    def update_criticNet(self,tensor_state,tensor_action,tensor_y):\n",
    "        x = self.forward(tensor_state,tensor_action)\n",
    "        loss = self.loss_fn(x,tensor_y)\n",
    "\n",
    "        # 反向传播\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1119c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从replaybuffer中采样，完成算法的一次迭代\n",
    "def update_Network(replaybuffer,Q,Q_t,u,u_t):\n",
    "    if len(replaybuffer.buffer)<BATCHSIZE:\n",
    "        return \n",
    "    # 形成 minibatch\n",
    "    tensor_y,tensor_action,tensor_state  = sample_Batch(replaybuffer,Q_t,u_t)\n",
    "\n",
    "    ## update Critic网络\n",
    "    Q.update_criticNet(tensor_state,tensor_action,tensor_y)\n",
    "\n",
    "    ## update Actor网络\n",
    "    # 使用Actor生成动作 u_action\n",
    "    u_action = u(tensor_state)\n",
    "    # 将采样的状态state和Actor生成的动作输入Critic网络计算评判结果\n",
    "    loss_u_Grad = - Q(tensor_state,u_action)\n",
    "    # 损失函数为取平均，对Q的评判结果的平均值取负，因为要做的是梯度上升\n",
    "    #loss_u_Grad =   Q_critic\n",
    "    loss_u_Grad = loss_u_Grad.mean()\n",
    "    \n",
    "    # 将反向传播通路上的Q网络的节点梯度清零\n",
    "    Q.optimizer.zero_grad()\n",
    "    # 反向传播\n",
    "    u.optimizer.zero_grad()\n",
    "    loss_u_Grad.backward()\n",
    "    u.optimizer.step()\n",
    "\n",
    "    ### update target网络\n",
    "    ## update Q_target网络\n",
    "    update_tNet(Q,Q_t,TAU)\n",
    "    \n",
    "    ## 更新u_target网络\n",
    "    update_tNet(u,u_t,TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新target网络，输入tau为更新权重\n",
    "def update_tNet(Net,Net_t,tau):\n",
    "    # 用于target网络的初始化\n",
    "    if tau == 0:\n",
    "        Net_t.load_state_dict(Net.state_dict())\n",
    "        return\n",
    "    \n",
    "    state_dict = Net.state_dict()\n",
    "    state_dict_t = Net_t.state_dict()\n",
    "    #print(state_dict)\n",
    "    for key in state_dict:\n",
    "        state_dict_t[key] = state_dict[key]*tau + (1-tau)*state_dict_t[key]\n",
    "    Net_t.load_state_dict(state_dict_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d27c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 Evaluation Average Reward: -0.02341581698887647\n",
      "episode:  100 Evaluation Average Reward: -0.09620635009481247\n",
      "episode:  200 Evaluation Average Reward: -0.08119758128627941\n",
      "episode:  300 Evaluation Average Reward: -0.023467650790592015\n",
      "episode:  400 Evaluation Average Reward: -0.13656248703983814\n",
      "episode:  500 Evaluation Average Reward: -0.05041257959871237\n",
      "episode:  600 Evaluation Average Reward: -0.07990899225254544\n",
      "episode:  700 Evaluation Average Reward: -0.9375851991693404\n",
      "episode:  800 Evaluation Average Reward: -0.6220744357565494\n",
      "episode:  900 Evaluation Average Reward: -0.1346398218981751\n",
      "episode:  1000 Evaluation Average Reward: 15.955824844118997\n",
      "episode:  1100 Evaluation Average Reward: 94.23124390777878\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    Mode = 'Train'\n",
    "    env = gym.make(ENV_NAME)\n",
    "    replaybuffer = ReplayBuffer(env,BUFFER_SIZE)\n",
    "    # 创建网络\n",
    "    Q = Critic_Net(env)\n",
    "    u = Policy_Net(env)\n",
    "    # target\n",
    "    Q_t = Critic_Net(env)\n",
    "    u_t = Policy_Net(env)\n",
    "\n",
    "    # 初始化target\n",
    "    update_tNet(Q,Q_t,0)\n",
    "    update_tNet(u,u_t,0)\n",
    "    if Mode == 'Train':\n",
    "        ave_reward = []\n",
    "        for episode in range(EPISODE):\n",
    "            # 初始化环境\n",
    "            state = env.reset()\n",
    "            acc_reward = 0 #累积奖赏\n",
    "            # 训练\n",
    "            for step in range(STEP):\n",
    "                # 产生动作\n",
    "                action = u.action_with_noise(state)\n",
    "                #观测\n",
    "                next_state,reward,done,_ = env.step(action.detach().numpy())\n",
    "                replaybuffer.append([state,action,reward,next_state])\n",
    "                #更新四个网络\n",
    "                update_Network(replaybuffer,Q,Q_t,u,u_t)\n",
    "                state = next_state\n",
    "                if done : \n",
    "                    break\n",
    "            if episode%100 == 0:#测试\n",
    "                acc_reward = 0\n",
    "                for i in range(TEST):\n",
    "                    state = env.reset()\n",
    "                    for step in range(STEP):\n",
    "                        action = u.action(state)\n",
    "                        state,reward,done,_ = env.step(action.detach().numpy())\n",
    "                        acc_reward += reward\n",
    "                        if done:\n",
    "                            break\n",
    "                ave_reward.append(acc_reward/TEST)\n",
    "                print('episode: ',episode,'Evaluation Average Reward:',acc_reward/TEST)\n",
    "                if acc_reward/TEST >93:\n",
    "                    #保存\n",
    "                    torch.save(u.state_dict(),SAVINGPATH+\"model-\"+str(acc_reward/TEST)+\".pth\")                \n",
    "                    break\n",
    "            \n",
    "    else :\n",
    "        u.load_state_dict()\n",
    "        state = env.reset()\n",
    "        acc_reward = 0\n",
    "        while True:\n",
    "            action = u.action(state)\n",
    "            state,reward,done,_ = env.step(action.detach().numpy())\n",
    "            acc_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(\"total reward: {}\".format(acc_reward))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ddd487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f9026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
